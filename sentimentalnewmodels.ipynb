{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T10:41:29.222787Z",
     "iopub.status.busy": "2026-01-06T10:41:29.222140Z",
     "iopub.status.idle": "2026-01-06T10:41:47.181560Z",
     "shell.execute_reply": "2026-01-06T10:41:47.180791Z",
     "shell.execute_reply.started": "2026-01-06T10:41:29.222752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. Install STABLE versions to fix \"cannot import name\" errors\n",
    "!pip install -q -U transformers==4.40.0 bitsandbytes==0.43.1 accelerate==0.29.3 peft==0.10.0 vaderSentiment\n",
    "\n",
    "# 2. Login to Hugging Face (Paste your token below)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# REPLACE 'hf_...' WITH YOUR ACTUAL TOKEN\n",
    "# Do not leave spaces around the string\n",
    "login(token=\"hugging face token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T10:41:54.497241Z",
     "iopub.status.busy": "2026-01-06T10:41:54.496350Z",
     "iopub.status.idle": "2026-01-06T10:42:11.753885Z",
     "shell.execute_reply": "2026-01-06T10:42:11.752885Z",
     "shell.execute_reply.started": "2026-01-06T10:41:54.497200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.43.0\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.32.0\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.1 in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
      "Collecting bitsandbytes>=0.43.1\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.43.0)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.43.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.32.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.32.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.43.0) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.43.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.43.0) (1.2.1rc0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.32.0) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.43.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.43.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.43.0) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.43.0) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.32.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.32.0) (3.0.3)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.43.1\n",
      "    Uninstalling bitsandbytes-0.43.1:\n",
      "      Successfully uninstalled bitsandbytes-0.43.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.29.3\n",
      "    Uninstalling accelerate-0.29.3:\n",
      "      Successfully uninstalled accelerate-0.29.3\n",
      "Successfully installed accelerate-1.12.0 bitsandbytes-0.49.0 tokenizers-0.22.2 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "# Force install the absolute latest versions to support Gemma 2 and Mistral v0.3\n",
    "!pip install -U \"transformers>=4.43.0\" \"accelerate>=0.32.0\" \"bitsandbytes>=0.43.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-06T10:41:01.324795Z",
     "iopub.status.busy": "2026-01-06T10:41:01.324499Z",
     "iopub.status.idle": "2026-01-06T10:41:16.865158Z",
     "shell.execute_reply": "2026-01-06T10:41:16.864120Z",
     "shell.execute_reply.started": "2026-01-06T10:41:01.324769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Checking for datasets...\n",
      "âœ… Labelled Data: Loaded 400 samples.\n",
      "âœ… Phrasebank: Loaded 4846 samples.\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.0/126.0 kB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/3456915945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- SCOREBOARD ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/3456915945.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Config for 4-bit loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     bnb_config = BitsAndBytesConfig(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mbnb_4bit_compute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unused kwargs: {list(kwargs.keys())}. These kwargs are not used in {self.__class__}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/quantization_config.py\u001b[0m in \u001b[0;36mpost_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bnb_4bit_use_double_quant must be a boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;34m\"0.39.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         ):\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;34m\"Version\"\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \"\"\"\n\u001b[0;32m--> 889\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mthereof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \"\"\"\n\u001b[0;32m--> 862\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODELS_TO_TEST = [\n",
    "    {\"name\": \"Llama-3-8B\", \"id\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \"type\": \"generative\"},\n",
    "    {\"name\": \"Gemma-2-9B\", \"id\": \"google/gemma-2-9b-it\", \"type\": \"generative\"},\n",
    "    {\"name\": \"Mistral-7B-v0.3\", \"id\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"type\": \"generative\"},\n",
    "    {\"name\": \"FinBERT\", \"id\": \"ProsusAI/finbert\", \"type\": \"bert\"},\n",
    "    {\"name\": \"VADER\", \"id\": \"vader\", \"type\": \"vader\"}\n",
    "]\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "def clean_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def predict_generative(text, model, tokenizer):\n",
    "    text_input = str(text)[:512]\n",
    "    prompt = (f\"classify the sentiment of this piece of news headline: {text_input}. \"\n",
    "              f\"Sentiment is 'positive', 'negative' or 'neutral'. \"\n",
    "              f\"Return only the sentiment of the news headline.\")\n",
    "    \n",
    "    # Check for chat template availability\n",
    "    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    else:\n",
    "        # Fallback for models without chat template (like base Mistral sometimes)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs, \n",
    "            max_new_tokens=10, \n",
    "            temperature=0.01, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    res = tokenizer.decode(output[0][len(inputs[0]):], skip_special_tokens=True).lower()\n",
    "    \n",
    "    if 'positive' in res: return 2\n",
    "    if 'negative' in res: return 0\n",
    "    return 1 # neutral\n",
    "\n",
    "def predict_finbert(text, model, tokenizer):\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    if prediction == 0: return 2 # Positive\n",
    "    if prediction == 1: return 0 # Negative\n",
    "    return 1 # Neutral\n",
    "\n",
    "def predict_vader(text, analyzer):\n",
    "    scores = analyzer.polarity_scores(str(text))\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05: return 2\n",
    "    if compound <= -0.05: return 0\n",
    "    return 1\n",
    "\n",
    "# --- ROBUST DATA LOADER ---\n",
    "def load_datasets():\n",
    "    path_lab = \"/kaggle/input/fullda/Fin_Cleaned.csv\"\n",
    "    path_phr = \"/kaggle/input/fullda/all-data.csv\"\n",
    "\n",
    "    print(f\"\\nğŸ” Checking for datasets...\")\n",
    "    df_lab = pd.DataFrame()\n",
    "    df_phr = pd.DataFrame()\n",
    "\n",
    "    # Load Labelled Data (Headers Exist)\n",
    "    if os.path.exists(path_lab):\n",
    "        try:\n",
    "            df_lab = pd.read_csv(path_lab, encoding='ISO-8859-1', on_bad_lines='skip')\n",
    "            df_lab['label'] = df_lab['Final Status'].astype(str).str.lower().str.strip().map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "            df_lab = df_lab.rename(columns={'Headline': 'text'})\n",
    "            df_lab = df_lab.dropna(subset=['label', 'text'])\n",
    "            print(f\"âœ… Labelled Data: Loaded {len(df_lab)} samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading Labelled Data: {e}\")\n",
    "    else:\n",
    "        print(f\"âŒ Labelled Data NOT FOUND at {path_lab}\")\n",
    "\n",
    "    # Load Phrasebank (No Headers)\n",
    "    if os.path.exists(path_phr):\n",
    "        try:\n",
    "            df_phr = pd.read_csv(path_phr, header=None, names=['label_str', 'text'], encoding='ISO-8859-1', on_bad_lines='skip')\n",
    "            df_phr['label'] = df_phr['label_str'].astype(str).str.lower().str.strip().map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "            df_phr = df_phr.dropna(subset=['label', 'text'])\n",
    "            print(f\"âœ… Phrasebank: Loaded {len(df_phr)} samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading Phrasebank: {e}\")\n",
    "    else:\n",
    "        print(f\"âŒ Phrasebank NOT FOUND at {path_phr}\")\n",
    "\n",
    "    return df_phr, df_lab\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_evaluation():\n",
    "    df_phr, df_lab = load_datasets()\n",
    "    if df_phr.empty and df_lab.empty: return pd.DataFrame()\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    # Initialize VADER\n",
    "    try:\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "    except:\n",
    "        subprocess.check_call([\"pip\", \"install\", \"vaderSentiment\"])\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Config for 4-bit loading\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    for m_cfg in MODELS_TO_TEST:\n",
    "        print(f\"\\n--- ğŸš€ Starting {m_cfg['name']} ---\")\n",
    "        clean_memory()\n",
    "        \n",
    "        try:\n",
    "            # 1. LOAD MODEL\n",
    "            if m_cfg['type'] == 'generative':\n",
    "                tokenizer = AutoTokenizer.from_pretrained(m_cfg['id'], token=True) # Uses logged-in token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    m_cfg['id'], \n",
    "                    quantization_config=bnb_config, \n",
    "                    device_map=\"auto\",\n",
    "                    token=True # Uses logged-in token\n",
    "                )\n",
    "                predict_func = lambda t: predict_generative(t, model, tokenizer)\n",
    "                \n",
    "            elif m_cfg['type'] == 'bert':\n",
    "                tokenizer = AutoTokenizer.from_pretrained(m_cfg['id'])\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(m_cfg['id'], device_map=\"auto\")\n",
    "                predict_func = lambda t: predict_finbert(t, model, tokenizer)\n",
    "                \n",
    "            elif m_cfg['type'] == 'vader':\n",
    "                predict_func = lambda t: predict_vader(t, analyzer)\n",
    "                model = None\n",
    "\n",
    "            results = {\"Model\": m_cfg['name']}\n",
    "            \n",
    "            # 2. EVALUATE\n",
    "            if not df_lab.empty:\n",
    "                y_true = df_lab['label'].values\n",
    "                y_pred = [predict_func(t) for t in tqdm(df_lab['text'], desc=\"Labelled\")]\n",
    "                results[\"Acc (Labelled)\"] = round(accuracy_score(y_true, y_pred), 4)\n",
    "            else:\n",
    "                results[\"Acc (Labelled)\"] = 0.0\n",
    "\n",
    "            if not df_phr.empty:\n",
    "                y_true = df_phr['label'].values\n",
    "                y_pred = [predict_func(t) for t in tqdm(df_phr['text'], desc=\"Phrasebank\")]\n",
    "                results[\"Acc (Phrasebank)\"] = round(accuracy_score(y_true, y_pred), 4)\n",
    "            else:\n",
    "                results[\"Acc (Phrasebank)\"] = 0.0\n",
    "            \n",
    "            # 3. SCORE\n",
    "            if df_phr.empty: results[\"Final Score\"] = results[\"Acc (Labelled)\"]\n",
    "            elif df_lab.empty: results[\"Final Score\"] = results[\"Acc (Phrasebank)\"]\n",
    "            else: results[\"Final Score\"] = (results[\"Acc (Labelled)\"] + results[\"Acc (Phrasebank)\"]) / 2\n",
    "            \n",
    "            results[\"Final Score\"] = round(results[\"Final Score\"], 4)\n",
    "            final_results.append(results)\n",
    "            print(f\"ğŸ“Š Result: {results['Final Score']}\")\n",
    "\n",
    "            if model: del model; del tokenizer\n",
    "            clean_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ FAILED {m_cfg['name']}: {str(e)}\")\n",
    "            clean_memory()\n",
    "            continue\n",
    "        \n",
    "    return pd.DataFrame(final_results).sort_values(\"Final Score\", ascending=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = run_evaluation()\n",
    "    print(\"\\n--- SCOREBOARD ---\")\n",
    "    if not df.empty:\n",
    "        print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9182920,
     "sourceId": 14379237,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
